{"GDELT": {"GlobalEventID": 940547455, "Day": 20200813, "MonthYear": 202008, "Year": 2020, "FractionDate": 2020.611, "Actor1Code": "GOV", "Actor1Name": "GOVERNMENT", "Actor1Type1Code": "GOV", "Actor2Code": "BUS", "Actor2Name": "INDUSTRY", "Actor2Type1Code": "BUS", "IsRootEvent": 0, "EventCode": 1721, "EventBaseCode": 172, "EventRootCode": 17, "QuadClass": 4, "GoldsteinScale": -5.0, "NumMentions": 6, "NumSources": 1, "NumArticles": 6, "AvgTone": -3.5230352303522996, "Actor1Geo_Type": 1, "Actor1Geo_Fullname": "Germany", "Actor1Geo_CountryCode": "GM", "Actor1Geo_ADM1Code": "GM", "Actor1Geo_Lat": 51.5, "Actor1Geo_Long": 10.5, "Actor1Geo_FeatureID": "GM", "Actor2Geo_Type": 1, "Actor2Geo_Fullname": "Germany", "Actor2Geo_CountryCode": "GM", "Actor2Geo_ADM1Code": "GM", "Actor2Geo_Lat": 51.5, "Actor2Geo_Long": 10.5, "Actor2Geo_FeatureID": "GM", "Action2Geo_Type": 1, "Action2Geo_Fullname": "Germany", "Action2Geo_CountryCode": "GM", "Action2Geo_ADM1Code": "GM", "Action2Geo_Lat": 51.5, "Action2Geo_Long": 10.5, "Action2Geo_FeatureID": "GM", "DATEADDED": 20200813134500, "SOURCEURL": "https://slate.com/technology/2020/08/gifct-content-moderation-free-speech-online.html"}, "ARTICLE": {"TITLE": "The GIFCT is the future of content moderation.", "TEXT": "This article is part of the Free Speech Project , a collaboration between Future Tense and the Tech, Law, & Security Program at American University Washington College of Law that examines the ways technology is influencing how we think about speech.\n\nLast October, a neo-Nazi livestreamed his attack on a synagogue in Halle, Germany. The video of the shooting, which killed two people, stayed on Twitch for more than an hour before it was removed. That\u2019s long enough for a recording to go viral\u2014but it never did. While users downloaded it and passed it around less moderated platforms, such as Telegram, the recording was stopped in its tracks on the major platforms: Facebook, Twitter, YouTube. The reason, Vice reported, is that Twitch was quick to share digital fingerprints, or hashes, of the video and its copies with these platforms and others. All Twitch had to do was upload the hashes to the database of the Global Internet Forum to Counter Terrorism, or, as it\u2019s been called, \u201cthe most underrated project in the future of free speech.\u201d\n\nThe GIFCT has gone largely unnoticed by the public since it was established in 2017 by Facebook, Microsoft, Twitter, and YouTube. One reason is that what it does is complicated. (Another may be its \u201cterrible acronym that no one can remember or pronounce,\u201d says Daphne Keller, director of the Program on Platform Regulation at Stanford\u2019s Cyber Policy Center.) Yet while onlookers pay close attention to Facebook\u2019s Oversight Board, or its civil rights audit, or Twitter\u2019s warning labels on President Donald Trump\u2019s tweets, the GIFCT is making some of the most consequential decisions in online speech governance without the scrutiny of the public.\n\nThe GIFCT started as an industry response to pressures from governments, and especially from European Union legislators, to assume greater responsibility in countering terrorism online after attacks in Paris and Brussels. The basic goal was to coordinate content removal across different services. So together, the tech giants set content norms by creating a hash database of what they consider \u201cviolent terrorist imagery and propaganda.\u201d This database, which serves as a kind of blacklist, helps other member sites\u2014many with modest content teams\u2014moderate their own platforms. At least 11 companies are members of the GIFCT, and an additional 13, including small operations such as Justpaste.it, have access to its database. Other than this basic structure, however, the inner workings of the GIFCT are opaque. Even as the coalition transitions to an independent organization, we still don\u2019t know how individual platforms use the database. Are uploads blocked immediately? Do the platforms check each piece of content? It\u2019s unclear.\n\nAt its best, a coalition like the GIFCT can serve as a way for services to leverage resources to counter extremist content. And since the GIFCT includes smaller platforms, it may allow those platforms to regulate content that would otherwise skate by unnoticed. The GIFCT also embraces a multistakeholder approach\u2014something that\u2019s often accepted as best practice in internet policymaking\u2014in its recently established Independent Advisory Committee, which includes members from civil society, governments, and intergovernmental bodies.\n\nYet the GIFCT\u2019s potential dangers, which were apparent to researchers from its inception, are just as clear. And they stem from the same places as its potential.\n\nThe GIFCT\u2019s structure typifies what Evelyn Douek, a doctoral student at Harvard Law School and affiliate at Harvard\u2019s Berkman Klein Center for Internet & Society, has termed content cartels, or \u201carrangements between platforms to work together to remove content or actors from their services without adequate oversight.\u201d Many of the problems with the GIFCT\u2019s arrangement lie in its opacity. None of the content decisions are transparent, and researchers don\u2019t have access to the hash database. As Keller recently laid out, the GIFCT sets the rules for \u201cviolent extremist\u201d speech in private, so it defines what is and isn\u2019t terrorist content without accountability. That\u2019s a serious problem, in part because content moderation mistakes and biases are inevitable. The GIFCT may very well be blocking satire, reporting on terrorism, and documentation of human rights abuses.\n\nAll of this isn\u2019t so different from what platforms do every day, free from constitutional or democratic legal restraints, but it\u2019s far more impactful, as the GIFCT\u2019s content decisions flow from large platforms to smaller ones. This dynamic isn\u2019t inherently problematic. In an ideal world, according to Keller, platforms would share the ability to find and assess content, while the actual judgments and enforcement of a speech policy would fall to each platform. \u201cIn theory, GIFCT should enable that because it doesn\u2019t force platforms to automatically take down whatever it flags,\u201d said Keller, \u201cbut it so far doesn\u2019t.\u201d Realistically, small platforms end up following the content recommendations handed down to them, since they don\u2019t have the resources to reevaluate content that may be flagged for review.\n\nNow, those concerns are compounded by the risks of extralegal censorship. Spurred by the Christchurch Call, a political summit of countries and tech companies to combat terrorist content online after the New Zealand mosque shootings, the GIFCT announced in 2019 that it would overhaul its internal structure. One component of that, the Independent Advisory Committee, includes government officials from Canada, France, Japan, Kenya, New Zealand, the United Kingdom, and the United States. Again, in theory the inclusion of government would make the GIFCT\u2019s work more democratically accountable. Yet it could also be an opportunity, as Emma Llans\u00f3, the director of the Free Expression Project at the Center for Democracy and Technology, said, for governments to have \u201cjust as much power as they usually do and none of the accountability.\u201d On July 30, the Center for Democracy and Technology joined 14 other human rights and digital rights organizations in a letter to Nick Rasmussen, the new executive director of the GIFCT, that outlined concerns about the coalition\u2019s lack of transparency and its use of government participation. \u201cCounter-terrorism programs and surveillance have violated the rights of Muslims, Arabs, and other groups around the world, and have been used by governments to silence civil society,\u201d the letter states. \u201cWe want to ensure that the boundaries between content moderation and counter terrorism are clear.\u201d\n\nGovernment participation in the GIFCT is a sort of Pandora\u2019s box. First, there\u2019s the question of whether platforms will decide to add content to the hash database under pressure from a government. There\u2019s also the issue of governments looking to voluntary industry bodies like the GIFCT in the future to push for censorship that wouldn\u2019t be lawful to pursue otherwise. Even government officials with the best of intentions, Llans\u00f3 said, will likely face tensions between their work on the GIFCT and what their governments are doing in, say, national legislative processes\u2014especially if they\u2019re considering an online hate speech law, such as Germany\u2019s Network Enforcement Act in 2017. Perhaps most importantly, added Llans\u00f3, this entanglement leaves us with a troubling question: How do people hold either the government or the companies accountable for the decisions that the GIFCT makes?\n\nDespite these criticisms of the GIFCT, few seem to think it\u2019s irredeemable. One general consensus is that the organization and its practices would be far more trustworthy if it heeded calls for transparency. In July, the GIFCT did release a transparency report, which details new initiatives that Keller finds promising, such as a tool that allows platforms to say that an image, video, or URL should not be in the database. That could be useful with parody, news content, or borderline content. But many fundamental aspects of the GIFCT remain unknown. Keller\u2019s transparency \u201cwish list\u201d includes granting researchers access to copies of the actual content that the GIFCT blocks, which would then allow researchers to assess bias and mistakes, and to release information about whether a user is notified (and given the chance to appeal) when the hash database blocks their content. Llans\u00f3, meanwhile, wants to know what exactly the Independent Advisory Committee does\u2014and to ensure that it remains truly advisory.\n\nWithout these changes, among others, researchers and civil rights advocates can\u2019t help but feel disappointment at the missed opportunity for the public to weigh in at a crucial moment in the formation of a new, semiprivate system of online speech governance. Decisions\u2014about internal governance, about content moderation\u2014are clearly being made behind closed doors, rather than through open, transparent, multistakeholder discussions. \u201cI feel like the evolution of GIFCT is a real illustration of just how slippery terms like platform responsibility or platform accountability turn out to be in practice,\u201d Keller said. When politicians demand initiatives like the GIFCT, she continued, it appears as though democratic governments are forcing platforms to act justly and lawfully. \u201cBut that\u2019s not what it\u2019s turned out to mean at all in practice,\u201d said Keller. \u201cIn practice, it meant that four platforms got together and made this totally opaque, really powerful system that\u2019s applying rules that aren\u2019t the law \u2026 to control online speech.\u201d\n\nFuture Tense is a partnership of Slate, New America, and Arizona State University that examines emerging technologies, public policy, and society.", "METADATA": {"viewport": "width=device-width,initial-scale=1,shrink-to-fit=no", "og": {"title": "The Future of Free Speech Online May Depend on This Database", "url": "https://slate.com/technology/2020/08/gifct-content-moderation-free-speech-online.html", "description": "GIFCT: terrible acronym. Very important coalition.", "image": "https://compote.slate.com/images/acbe07a3-7df1-4de6-8507-7e7771cad977.jpeg?width=780&height=520&rect=1560x1040&offset=0x0", "site_name": "Slate Magazine", "type": "article"}, "twitter": {"title": "The Future of Free Speech Online May Depend on This Database", "description": "GIFCT: terrible acronym. Very important coalition.", "image": "https://compote.slate.com/images/acbe07a3-7df1-4de6-8507-7e7771cad977.jpeg?width=780&height=520&rect=1560x1040&offset=0x0", "card": "summary_large_image", "site": "@slate"}, "article": {"published_time": "2020-08-13T13:00:00.000Z", "tag": "twitter, facebook, youtube, free-speech-project, terrorism, social-media", "publisher": "https://www.facebook.com/Slate"}, "description": "GIFCT: terrible acronym. Very important coalition.", "news_keywords": "twitter, facebook, youtube, free-speech-project, terrorism, social-media", "author": "Chloe Hadavas", "fb": {"app_id": 142011022527753}, "apple-mobile-web-app-title": "Slate", "application-name": "Slate", "msapplication-TileColor": "#660033", "msapplication-TileImage": "https://slate.com/media/sites/slate-com/icon.144x144.png", "theme-color": "#2c0022", "dfp-cache-buster": "7b1b9fb6f55280c03edfe3d6def54723514811de"}}, "LABEL": {"WANT_ON_MAP": false, "NOTES": ["No Coronavirus Keywords"]}}